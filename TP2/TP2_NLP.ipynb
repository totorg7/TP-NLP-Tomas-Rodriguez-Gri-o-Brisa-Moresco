{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Trabajo Practico final NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrante: Tomás Rodríguez Griñó\n",
    "\n",
    "### Legajo: R-4643/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importacion de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.11 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0)\n",
      "ERROR: No matching distribution found for tensorflow==2.11\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que debemos realizar, es una busqueda de datos para darle contexto a el chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descripción: The heron flies over the Himeji sky while the Daimyo, from the top of the castle, watches his servants move. Gardeners tend the pond, where the koi carp live, warriors stand guard on the walls, and courtiers crowd the gates, pining for an audience that brings them closer to the innermost circles of the court. When night falls, the lanterns are lit and the workers return to their clan.\n",
      "In The White Castle, players will control one of these clans in order to score more victory points than the rest. To do so, they must amass influence in the court, manage resources boldly, and place their workers in the right place at the right time. The authors are Sheila Santos and Israel Cendrero, the duo known as Llama Dice who also designed the successful The Red Cathedral with Devir. In this case, we leave the Moscow of Ivan the Terrible behind to explore the most imposing fortress in modern Japan, Himeji Castle, where the banner of the Sakai clan flies under the orders of Daimyo Sakai Tadakiyo.\n",
      "The White Castle is a Euro type game with mechanics of resource management, worker placement and dice placement to carry out actions. During the game, over three rounds, players will send members of their clan to tend the gardens, defend the castle or progress up the social ladder of the nobility. At the end of the match, these will award players victory points in a variety of ways.\n",
      "No se encontró el 'ul' de premios.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Configuración de Selenium\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "try:\n",
    "    # URL de la página a scrapear\n",
    "    url = \"https://boardgamegeek.com/boardgame/371942/the-white-castle\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Extraer la descripción\n",
    "    wait = WebDriverWait(driver, 60)\n",
    "    description_div = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"game-description-body\")))\n",
    "    inner_description_div = description_div.find_element(By.CLASS_NAME, \"ng-binding\")\n",
    "    description_text = inner_description_div.text\n",
    "    print(\"Descripción:\", description_text)\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se utiliza web scrapping con Selenium para extraer tanto informacion como consultas y opiniones de usuarios de la pagina Misut Meeple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text():\n",
    "    # URL de la página\n",
    "    url = \"https://misutmeeple.com/2023/11/resena-the-white-castle/\"\n",
    "\n",
    "    # Configuración de Selenium\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        # Navegar a la URL\n",
    "        driver.get(url)\n",
    "\n",
    "        # Esperar a que el contenido se cargue\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "        # Extraer texto de <p> y encabezados\n",
    "        paragraphs = driver.find_elements(By.TAG_NAME, 'p')\n",
    "        headers = driver.find_elements(By.TAG_NAME, 'h1') + driver.find_elements(By.TAG_NAME, 'h2') + driver.find_elements(By.TAG_NAME, 'h3')\n",
    "\n",
    "        # Concatenar texto\n",
    "        all_text = \"\"\n",
    "        for element in paragraphs + headers:\n",
    "            all_text += element.text + \"\\n\"\n",
    "\n",
    "        return all_text\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "text = scrape_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 512, which is longer than the specified 500\n",
      "Created a chunk of size 1090, which is longer than the specified 500\n",
      "Created a chunk of size 909, which is longer than the specified 500\n",
      "Created a chunk of size 949, which is longer than the specified 500\n",
      "Created a chunk of size 643, which is longer than the specified 500\n",
      "Created a chunk of size 838, which is longer than the specified 500\n",
      "Created a chunk of size 822, which is longer than the specified 500\n",
      "Created a chunk of size 738, which is longer than the specified 500\n",
      "Created a chunk of size 518, which is longer than the specified 500\n",
      "Created a chunk of size 549, which is longer than the specified 500\n",
      "Created a chunk of size 508, which is longer than the specified 500\n",
      "Created a chunk of size 593, which is longer than the specified 500\n",
      "Created a chunk of size 547, which is longer than the specified 500\n",
      "Created a chunk of size 584, which is longer than the specified 500\n",
      "Created a chunk of size 523, which is longer than the specified 500\n",
      "Created a chunk of size 585, which is longer than the specified 500\n",
      "Created a chunk of size 567, which is longer than the specified 500\n",
      "Created a chunk of size 859, which is longer than the specified 500\n",
      "Created a chunk of size 559, which is longer than the specified 500\n",
      "Created a chunk of size 582, which is longer than the specified 500\n",
      "Created a chunk of size 884, which is longer than the specified 500\n",
      "Created a chunk of size 844, which is longer than the specified 500\n",
      "Created a chunk of size 688, which is longer than the specified 500\n",
      "Created a chunk of size 990, which is longer than the specified 500\n",
      "Created a chunk of size 591, which is longer than the specified 500\n",
      "Created a chunk of size 816, which is longer than the specified 500\n",
      "Created a chunk of size 821, which is longer than the specified 500\n",
      "Created a chunk of size 518, which is longer than the specified 500\n",
      "Created a chunk of size 520, which is longer than the specified 500\n",
      "Created a chunk of size 592, which is longer than the specified 500\n",
      "Created a chunk of size 1555, which is longer than the specified 500\n",
      "Created a chunk of size 692, which is longer than the specified 500\n",
      "Created a chunk of size 547, which is longer than the specified 500\n",
      "Created a chunk of size 564, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "def split_text(text):\n",
    "    # Configuración del splitter\n",
    "    splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=500, chunk_overlap=50)\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "chunks = split_text(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Inicializa el modelo y el tokenizer de Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def embed_documents(texts):\n",
    "    # Tokeniza y genera los embeddings\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        # Genera los embeddings promediando las representaciones de las últimas capas\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Generar los embeddings de los textos\n",
    "embeddings = embed_documents(chunks)\n",
    "\n",
    "# Verificar que la longitud de embeddings coincida con la longitud de los textos\n",
    "assert len(chunks) == len(embeddings), \"Las longitudes de los textos y los embeddings no coinciden.\"\n",
    "\n",
    "# Asignar un ID único a cada documento\n",
    "ids_textos = [f\"doc{i}\" for i in range(1, len(chunks) + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guaradado en ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: doc1\n",
      "Insert of existing embedding ID: doc2\n",
      "Insert of existing embedding ID: doc3\n",
      "Insert of existing embedding ID: doc4\n",
      "Add of existing embedding ID: doc1\n",
      "Add of existing embedding ID: doc2\n",
      "Add of existing embedding ID: doc3\n",
      "Add of existing embedding ID: doc4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings guardados en ChromaDB con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Configuración inicial de ChromaDB\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Crear o acceder a una colección en ChromaDB\n",
    "collection = client.get_or_create_collection(\"mis-documentos\")\n",
    "\n",
    "# Guardar los documentos y embeddings en ChromaDB\n",
    "collection.add(\n",
    "    documents=chunks,              # Los documentos originales\n",
    "    metadatas=[{\"source\": \"fuente1\"} for _ in chunks],  # Metadata opcional, en este caso solo una fuente\n",
    "    ids=ids_textos,                # IDs para los documentos\n",
    "    embeddings=embeddings.tolist() # Los embeddings generados previamente\n",
    ")\n",
    "\n",
    "print(\"Embeddings guardados en ChromaDB con éxito.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
